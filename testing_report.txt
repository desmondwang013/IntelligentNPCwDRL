============================================================
CURRICULUM V2 SETUP
============================================================
World sizes: [8, 16, 32, 64]
Initial weights: {8: 0.7, 16: 0.2, 32: 0.08, 64: 0.02}
Warmup episodes: 100
Success radius: 5.0 -> 2.0
Shift threshold: 40% success rate
============================================================
Using DummyVecEnv with 4 environments
Using cpu device

Starting training for 500000 timesteps...
Reward focus: distance-progress (getting closer = good)

Logging to logs/curriculum_v2\PPO_2
[Debug] Episode 1: world=8, reason=INTENT_COMPLETED, reward=9.39
[Debug] Episode 2: world=8, reason=unknown, reward=-65.43
[Debug] Episode 3: world=8, reason=unknown, reward=-51.96
[Debug] Episode 4: world=8, reason=unknown, reward=-69.14
[Debug] Episode 5: world=32, reason=unknown, reward=-135.00
Eval num_timesteps=5000, episode_reward=-117.18 +/- 65.50
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -31.9    |
| time/              |          |
|    fps             | 727      |
|    iterations      | 1        |
|    time_elapsed    | 11       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-93.00 +/- 76.03
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -93         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017532043 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -0.0228     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.73        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 7.23        |
-----------------------------------------
New best mean reward!
[Curriculum] Episode 50 (warmup: 50 remaining)
Eval num_timesteps=15000, episode_reward=-76.72 +/- 79.89
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -76.7    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | -9.53    |
| time/              |          |
|    fps             | 448      |
|    iterations      | 2        |
|    time_elapsed    | 36       |
|    total_timesteps | 16384    |
---------------------------------
[Curriculum] Episode 100: success=0.0%, radius=4.7, weights=[8:70% 16:20% 32:8% 64:2%]
Eval num_timesteps=20000, episode_reward=-90.62 +/- 78.95
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -90.6       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.022783164 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.104       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.9         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 12          |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -3.43    |
| time/              |          |
|    fps             | 449      |
|    iterations      | 3        |
|    time_elapsed    | 54       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-45.05 +/- 72.69
Episode length: 351.00 +/- 227.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -45         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.017252784 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.67        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 7.89        |
-----------------------------------------
New best mean reward!
[Curriculum] Episode 150: success=66.7%, radius=4.5, weights=[8:70% 16:20% 32:8% 64:2%]
Eval num_timesteps=30000, episode_reward=-11.68 +/- 48.64
Episode length: 251.60 +/- 248.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | -11.7    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -8.62    |
| time/              |          |
|    fps             | 434      |
|    iterations      | 4        |
|    time_elapsed    | 75       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-78.29 +/- 73.59
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -78.3       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.019235855 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.84        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 8.52        |
-----------------------------------------
[Curriculum] Episode 200: success=68.3%, radius=4.4, weights=[8:68% 16:20% 32:8% 64:5%]
Eval num_timesteps=40000, episode_reward=-80.00 +/- 75.00
Episode length: 500.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | -80      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | -4.89    |
| time/              |          |
|    fps             | 402      |
|    iterations      | 5        |
|    time_elapsed    | 101      |
|    total_timesteps | 40960    |
---------------------------------
[Curriculum] Episode 250: success=71.5%, radius=4.2, weights=[8:58% 16:26% 32:10% 64:6%]
Eval num_timesteps=45000, episode_reward=-30.62 +/- 62.51
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -30.6       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019086104 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 5.97        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | -0.062   |
| time/              |          |
|    fps             | 408      |
|    iterations      | 6        |
|    time_elapsed    | 120      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-93.50 +/- 75.43
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -93.5       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019828795 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.05        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 7.07        |
-----------------------------------------
[Curriculum] Episode 300: success=71.0%, radius=4.1, weights=[8:50% 16:30% 32:13% 64:8%]
Eval num_timesteps=55000, episode_reward=-93.47 +/- 72.37
Episode length: 500.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | -93.5    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | -5.15    |
| time/              |          |
|    fps             | 398      |
|    iterations      | 7        |
|    time_elapsed    | 143      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-45.72 +/- 71.78
Episode length: 350.60 +/- 228.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 351        |
|    mean_reward          | -45.7      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02102868 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 5.96       |
----------------------------------------
[Curriculum] Episode 350: success=68.5%, radius=4.0, weights=[8:43% 16:32% 32:15% 64:10%]
Eval num_timesteps=65000, episode_reward=-47.59 +/- 70.15
Episode length: 350.60 +/- 228.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -47.6    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 394      |
|    iterations      | 8        |
|    time_elapsed    | 166      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-32.87 +/- 61.23
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -32.9       |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.024286939 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 4.02        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -14.6    |
| time/              |          |
|    fps             | 400      |
|    iterations      | 9        |
|    time_elapsed    | 183      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-45.50 +/- 71.95
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -45.5       |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.017544296 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.34        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 5.05        |
-----------------------------------------
[Curriculum] Episode 400: success=62.0%, radius=3.8, weights=[8:38% 16:33% 32:17% 64:12%]
Eval num_timesteps=80000, episode_reward=-80.09 +/- 75.80
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -80.1    |
| time/              |          |
|    total_timesteps | 80000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    fps             | 400      |
|    iterations      | 10       |
|    time_elapsed    | 204      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-44.52 +/- 72.55
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -44.5       |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.021065244 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.173       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.51        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 7.79        |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=-31.59 +/- 62.18
Episode length: 350.60 +/- 228.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -31.6    |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | -1.26    |
| time/              |          |
|    fps             | 398      |
|    iterations      | 11       |
|    time_elapsed    | 225      |
|    total_timesteps | 90112    |
---------------------------------
[Curriculum] Episode 450: success=54.5%, radius=3.6, weights=[8:35% 16:33% 32:19% 64:14%]
Eval num_timesteps=95000, episode_reward=-76.62 +/- 78.52
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -76.6       |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.020328812 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.19        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 9.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | -1.46    |
| time/              |          |
|    fps             | 402      |
|    iterations      | 12       |
|    time_elapsed    | 243      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-60.33 +/- 77.44
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -60.3       |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.021187685 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 6.45        |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=-31.87 +/- 61.35
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -31.9    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | -0.38    |
| time/              |          |
|    fps             | 402      |
|    iterations      | 13       |
|    time_elapsed    | 264      |
|    total_timesteps | 106496   |
---------------------------------
[Curriculum] Episode 500: success=46.5%, radius=3.5, weights=[8:33% 16:33% 32:19% 64:15%]
Eval num_timesteps=110000, episode_reward=-15.55 +/- 46.95
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -15.6       |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.018986922 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.11        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 6.54        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | -4.38    |
| time/              |          |
|    fps             | 407      |
|    iterations      | 14       |
|    time_elapsed    | 281      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-77.72 +/- 75.87
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -77.7       |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.021854129 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 5.3         |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=-43.97 +/- 72.27
Episode length: 300.80 +/- 243.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | -44      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 406      |
|    iterations      | 15       |
|    time_elapsed    | 302      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-43.26 +/- 71.55
Episode length: 300.80 +/- 243.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | -43.3       |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.021155622 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.12        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 6.96        |
-----------------------------------------
[Curriculum] Episode 550: success=43.0%, radius=3.3, weights=[8:32% 16:33% 32:20% 64:15%]
Eval num_timesteps=130000, episode_reward=-109.07 +/- 68.67
Episode length: 500.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | -6.94    |
| time/              |          |
|    fps             | 404      |
|    iterations      | 16       |
|    time_elapsed    | 323      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-102.71 +/- 65.95
Episode length: 500.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | -103       |
| time/                   |            |
|    total_timesteps      | 135000     |
| train/                  |            |
|    approx_kl            | 0.02078749 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 0.246      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0261    |
|    value_loss           | 8.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | -8.73    |
| time/              |          |
|    fps             | 408      |
|    iterations      | 17       |
|    time_elapsed    | 341      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-61.37 +/- 75.77
Episode length: 400.40 +/- 199.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 400        |
|    mean_reward          | -61.4      |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.01676903 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.34       |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 4.03       |
----------------------------------------
[Curriculum] Episode 600: success=41.5%, radius=3.2, weights=[8:32% 16:33% 32:20% 64:15%]
Eval num_timesteps=145000, episode_reward=-93.56 +/- 75.35
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -93.6    |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | -0.171   |
| time/              |          |
|    fps             | 406      |
|    iterations      | 18       |
|    time_elapsed    | 362      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-73.93 +/- 71.50
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -73.9       |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.021055184 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.58        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 6.47        |
-----------------------------------------
Eval num_timesteps=155000, episode_reward=-91.25 +/- 76.58
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -91.3    |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 404      |
|    iterations      | 19       |
|    time_elapsed    | 385      |
|    total_timesteps | 155648   |
---------------------------------
[Curriculum] Episode 650: success=39.5%, radius=3.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=160000, episode_reward=-106.68 +/- 71.72
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -107       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.02151078 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.62       |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 7.38       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | -4.03    |
| time/              |          |
|    fps             | 406      |
|    iterations      | 20       |
|    time_elapsed    | 403      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-51.47 +/- 69.43
Episode length: 400.40 +/- 199.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 400        |
|    mean_reward          | -51.5      |
| time/                   |            |
|    total_timesteps      | 165000     |
| train/                  |            |
|    approx_kl            | 0.01959571 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.43       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.12       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 8.69       |
----------------------------------------
Eval num_timesteps=170000, episode_reward=-48.17 +/- 70.06
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -48.2    |
| time/              |          |
|    total_timesteps | 170000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 323      |
|    ep_rew_mean     | -3.61    |
| time/              |          |
|    fps             | 404      |
|    iterations      | 21       |
|    time_elapsed    | 425      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=175000, episode_reward=-64.15 +/- 72.84
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -64.1       |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.024964776 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.11        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 6.22        |
-----------------------------------------
[Curriculum] Episode 700: success=40.5%, radius=2.9, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=180000, episode_reward=-74.44 +/- 79.58
Episode length: 350.60 +/- 228.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -74.4    |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | -4.84    |
| time/              |          |
|    fps             | 403      |
|    iterations      | 22       |
|    time_elapsed    | 446      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=185000, episode_reward=-91.49 +/- 77.62
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -91.5       |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.022235572 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 5.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 407      |
|    iterations      | 23       |
|    time_elapsed    | 462      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-75.08 +/- 79.61
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -75.1       |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.019319734 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 4.94        |
-----------------------------------------
Eval num_timesteps=195000, episode_reward=-79.75 +/- 73.56
Episode length: 500.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | -79.7    |
| time/              |          |
|    total_timesteps | 195000   |
---------------------------------
[Curriculum] Episode 750: success=34.0%, radius=2.8, weights=[8:31% 16:33% 32:20% 64:16%]
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    fps             | 406      |
|    iterations      | 24       |
|    time_elapsed    | 483      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-91.87 +/- 77.44
Episode length: 400.40 +/- 199.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 400        |
|    mean_reward          | -91.9      |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.02215825 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.58       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 6.59       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 377      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 409      |
|    iterations      | 25       |
|    time_elapsed    | 499      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-76.57 +/- 77.50
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -76.6       |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.022022512 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 6.59        |
-----------------------------------------
Eval num_timesteps=210000, episode_reward=-106.54 +/- 72.87
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 409      |
|    iterations      | 26       |
|    time_elapsed    | 520      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-108.38 +/- 68.53
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -108        |
| time/                   |             |
|    total_timesteps      | 215000      |
| train/                  |             |
|    approx_kl            | 0.026901709 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.89        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 6.26        |
-----------------------------------------
[Curriculum] Episode 800: success=30.0%, radius=2.6, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=220000, episode_reward=-75.90 +/- 77.51
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -75.9    |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    fps             | 408      |
|    iterations      | 27       |
|    time_elapsed    | 540      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-119.00 +/- 62.70
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -119       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.02041636 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.69       |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0237    |
|    value_loss           | 5.01       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | -21.6    |
| time/              |          |
|    fps             | 411      |
|    iterations      | 28       |
|    time_elapsed    | 557      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-123.56 +/- 62.96
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -124        |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.020294353 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 3.22        |
-----------------------------------------
Eval num_timesteps=235000, episode_reward=-93.56 +/- 75.35
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -93.6    |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
[Curriculum] Episode 850: success=26.5%, radius=2.5, weights=[8:31% 16:33% 32:20% 64:16%]
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 410      |
|    iterations      | 29       |
|    time_elapsed    | 579      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-45.91 +/- 71.62
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -45.9       |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.023405183 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 4.95        |
-----------------------------------------
Eval num_timesteps=245000, episode_reward=-80.18 +/- 74.19
Episode length: 500.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | -80.2    |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 385      |
|    ep_rew_mean     | -16      |
| time/              |          |
|    fps             | 409      |
|    iterations      | 30       |
|    time_elapsed    | 600      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-77.00 +/- 78.45
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -77         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.020280503 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.05        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 6.57        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 411      |
|    iterations      | 31       |
|    time_elapsed    | 616      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-59.56 +/- 75.15
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -59.6       |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.025308032 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 5.16        |
-----------------------------------------
[Curriculum] Episode 900: success=20.5%, radius=2.3, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=260000, episode_reward=-44.17 +/- 70.91
Episode length: 350.90 +/- 227.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -44.2    |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | -13      |
| time/              |          |
|    fps             | 411      |
|    iterations      | 32       |
|    time_elapsed    | 636      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=265000, episode_reward=-91.99 +/- 75.66
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -92         |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.021603392 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.67        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0295     |
|    value_loss           | 7.99        |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=-92.05 +/- 77.08
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -92      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    fps             | 411      |
|    iterations      | 33       |
|    time_elapsed    | 657      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-105.48 +/- 69.47
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -105        |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.022907432 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 5.68        |
-----------------------------------------
[Curriculum] Episode 950: success=24.0%, radius=2.2, weights=[8:31% 16:33% 32:20% 64:16%]
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 413      |
|    iterations      | 34       |
|    time_elapsed    | 674      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-89.85 +/- 79.96
Episode length: 350.90 +/- 227.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -89.8       |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.019846633 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.58        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 7.76        |
-----------------------------------------
Eval num_timesteps=285000, episode_reward=-75.70 +/- 79.66
Episode length: 401.00 +/- 198.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | -75.7    |
| time/              |          |
|    total_timesteps | 285000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    fps             | 412      |
|    iterations      | 35       |
|    time_elapsed    | 694      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-91.89 +/- 75.59
Episode length: 450.60 +/- 148.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 451         |
|    mean_reward          | -91.9       |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.022678083 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.69        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 6.82        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 387      |
|    ep_rew_mean     | -8.94    |
| time/              |          |
|    fps             | 414      |
|    iterations      | 36       |
|    time_elapsed    | 711      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-108.38 +/- 70.92
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -108       |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.02487149 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.39       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 5.94       |
----------------------------------------
[Curriculum] Episode 1000: success=24.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=300000, episode_reward=-58.91 +/- 78.60
Episode length: 300.80 +/- 243.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | -58.9    |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    fps             | 414      |
|    iterations      | 37       |
|    time_elapsed    | 730      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-57.73 +/- 78.63
Episode length: 400.80 +/- 198.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 401         |
|    mean_reward          | -57.7       |
| time/                   |             |
|    total_timesteps      | 305000      |
| train/                  |             |
|    approx_kl            | 0.022989567 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.42        |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 5.91        |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=-91.75 +/- 77.57
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -91.7    |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 414      |
|    iterations      | 38       |
|    time_elapsed    | 751      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-74.68 +/- 77.90
Episode length: 350.60 +/- 228.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 351        |
|    mean_reward          | -74.7      |
| time/                   |            |
|    total_timesteps      | 315000     |
| train/                  |            |
|    approx_kl            | 0.02263587 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 7.1        |
----------------------------------------
[Curriculum] Episode 1050: success=21.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | -22.7    |
| time/              |          |
|    fps             | 416      |
|    iterations      | 39       |
|    time_elapsed    | 767      |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-43.67 +/- 73.06
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -43.7       |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.024360718 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.48        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 5.76        |
-----------------------------------------
Eval num_timesteps=325000, episode_reward=-30.58 +/- 62.53
Episode length: 350.60 +/- 228.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 435      |
|    ep_rew_mean     | -26.5    |
| time/              |          |
|    fps             | 416      |
|    iterations      | 40       |
|    time_elapsed    | 787      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-32.23 +/- 61.54
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -32.2       |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.025054066 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.03        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 5.03        |
-----------------------------------------
Eval num_timesteps=335000, episode_reward=-47.12 +/- 70.83
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -47.1    |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 435      |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 415      |
|    iterations      | 41       |
|    time_elapsed    | 807      |
|    total_timesteps | 335872   |
---------------------------------
[Curriculum] Episode 1100: success=21.5%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=340000, episode_reward=-74.73 +/- 78.54
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -74.7       |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.022125399 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 8.09        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 432      |
|    ep_rew_mean     | -25.8    |
| time/              |          |
|    fps             | 417      |
|    iterations      | 42       |
|    time_elapsed    | 823      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=345000, episode_reward=-61.35 +/- 75.55
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -61.3       |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.024962092 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 3.58        |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=-47.78 +/- 69.34
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -47.8    |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 430      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    fps             | 417      |
|    iterations      | 43       |
|    time_elapsed    | 843      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=355000, episode_reward=-77.12 +/- 78.03
Episode length: 400.60 +/- 198.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 401        |
|    mean_reward          | -77.1      |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.02076986 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 5.7        |
----------------------------------------
Eval num_timesteps=360000, episode_reward=-76.91 +/- 78.24
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -76.9    |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    fps             | 417      |
|    iterations      | 44       |
|    time_elapsed    | 863      |
|    total_timesteps | 360448   |
---------------------------------
[Curriculum] Episode 1150: success=17.5%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=365000, episode_reward=-50.47 +/- 69.85
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -50.5       |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.021694772 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.69        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 4.9         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 432      |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    fps             | 418      |
|    iterations      | 45       |
|    time_elapsed    | 880      |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-77.12 +/- 78.04
Episode length: 400.40 +/- 199.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | -77.1       |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.022441305 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49        |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 4.71        |
-----------------------------------------
Eval num_timesteps=375000, episode_reward=-23.59 +/- 52.08
Episode length: 350.90 +/- 227.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -23.6    |
| time/              |          |
|    total_timesteps | 375000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 418      |
|    iterations      | 46       |
|    time_elapsed    | 900      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-48.06 +/- 69.06
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -48.1      |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.02441705 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 5.91       |
----------------------------------------
[Curriculum] Episode 1200: success=14.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=385000, episode_reward=-60.90 +/- 74.83
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -60.9    |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 452      |
|    ep_rew_mean     | -9.9     |
| time/              |          |
|    fps             | 418      |
|    iterations      | 47       |
|    time_elapsed    | 920      |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-75.70 +/- 79.78
Episode length: 350.60 +/- 228.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 351        |
|    mean_reward          | -75.7      |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.02410546 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.95       |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 5.32       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 420      |
|    iterations      | 48       |
|    time_elapsed    | 935      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-99.31 +/- 70.59
Episode length: 400.40 +/- 199.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 400        |
|    mean_reward          | -99.3      |
| time/                   |            |
|    total_timesteps      | 395000     |
| train/                  |            |
|    approx_kl            | 0.02101418 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.7        |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 4.76       |
----------------------------------------
Eval num_timesteps=400000, episode_reward=-29.24 +/- 63.21
Episode length: 300.80 +/- 243.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | -29.2    |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | -20.1    |
| time/              |          |
|    fps             | 420      |
|    iterations      | 49       |
|    time_elapsed    | 955      |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-77.50 +/- 77.15
Episode length: 450.30 +/- 149.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -77.5       |
| time/                   |             |
|    total_timesteps      | 405000      |
| train/                  |             |
|    approx_kl            | 0.030184707 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 4.66        |
-----------------------------------------
[Curriculum] Episode 1250: success=13.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    fps             | 421      |
|    iterations      | 50       |
|    time_elapsed    | 971      |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-125.78 +/- 49.49
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -126       |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.02158086 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.41      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0276    |
|    value_loss           | 5.7        |
----------------------------------------
Eval num_timesteps=415000, episode_reward=-56.61 +/- 70.91
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -56.6    |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 456      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    fps             | 421      |
|    iterations      | 51       |
|    time_elapsed    | 991      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-118.78 +/- 58.77
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.019462932 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.97        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 5.36        |
-----------------------------------------
Eval num_timesteps=425000, episode_reward=-33.46 +/- 60.93
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | -22.4    |
| time/              |          |
|    fps             | 420      |
|    iterations      | 52       |
|    time_elapsed    | 1013     |
|    total_timesteps | 425984   |
---------------------------------
[Curriculum] Episode 1300: success=15.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=430000, episode_reward=-109.33 +/- 70.72
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -109        |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.021306204 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 5.56        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | -18.8    |
| time/              |          |
|    fps             | 421      |
|    iterations      | 53       |
|    time_elapsed    | 1030     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=435000, episode_reward=-85.15 +/- 75.55
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -85.1       |
| time/                   |             |
|    total_timesteps      | 435000      |
| train/                  |             |
|    approx_kl            | 0.023213413 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 4.39        |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=-74.03 +/- 80.03
Episode length: 300.80 +/- 243.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | -74      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | -18.4    |
| time/              |          |
|    fps             | 421      |
|    iterations      | 54       |
|    time_elapsed    | 1050     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=445000, episode_reward=-29.32 +/- 63.21
Episode length: 300.80 +/- 243.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | -29.3       |
| time/                   |             |
|    total_timesteps      | 445000      |
| train/                  |             |
|    approx_kl            | 0.026786117 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.33        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 5.64        |
-----------------------------------------
[Curriculum] Episode 1350: success=17.5%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=450000, episode_reward=-57.86 +/- 69.58
Episode length: 450.20 +/- 149.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | -57.9    |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    fps             | 421      |
|    iterations      | 55       |
|    time_elapsed    | 1069     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=455000, episode_reward=-107.03 +/- 70.20
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -107        |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.023848595 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.85        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 5.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    fps             | 422      |
|    iterations      | 56       |
|    time_elapsed    | 1086     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-57.44 +/- 68.58
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -57.4       |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.021314267 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 3.93        |
-----------------------------------------
Eval num_timesteps=465000, episode_reward=-76.60 +/- 77.65
Episode length: 400.40 +/- 199.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | -76.6    |
| time/              |          |
|    total_timesteps | 465000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 422      |
|    iterations      | 57       |
|    time_elapsed    | 1106     |
|    total_timesteps | 466944   |
---------------------------------
[Curriculum] Episode 1400: success=19.0%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=470000, episode_reward=-87.80 +/- 73.05
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -87.8       |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.022492556 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 3.86        |
-----------------------------------------
Eval num_timesteps=475000, episode_reward=-16.46 +/- 48.96
Episode length: 301.50 +/- 243.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | -16.5    |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | -22      |
| time/              |          |
|    fps             | 422      |
|    iterations      | 58       |
|    time_elapsed    | 1125     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-38.66 +/- 63.85
Episode length: 350.60 +/- 228.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | -38.7       |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.022897324 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 4.48        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    fps             | 422      |
|    iterations      | 59       |
|    time_elapsed    | 1142     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-62.91 +/- 74.60
Episode length: 450.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | -62.9      |
| time/                   |            |
|    total_timesteps      | 485000     |
| train/                  |            |
|    approx_kl            | 0.02111792 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0294    |
|    value_loss           | 5.93       |
----------------------------------------
[Curriculum] Episode 1450: success=22.5%, radius=2.0, weights=[8:31% 16:33% 32:20% 64:16%]
Eval num_timesteps=490000, episode_reward=-69.38 +/- 75.33
Episode length: 350.60 +/- 228.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | -69.4    |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 423      |
|    ep_rew_mean     | -24.8    |
| time/              |          |
|    fps             | 422      |
|    iterations      | 60       |
|    time_elapsed    | 1162     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-78.10 +/- 77.02
Episode length: 450.20 +/- 149.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 450         |
|    mean_reward          | -78.1       |
| time/                   |             |
|    total_timesteps      | 495000      |
| train/                  |             |
|    approx_kl            | 0.018640926 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 4.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | -25.6    |
| time/              |          |
|    fps             | 423      |
|    iterations      | 61       |
|    time_elapsed    | 1178     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-65.07 +/- 73.57
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | -65.1       |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.023749888 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 6.55        |
-----------------------------------------
Eval num_timesteps=505000, episode_reward=-76.26 +/- 77.45
 100%  507,904/500,000  [ 0:19:48 < 0:00:00 , 488 it/s ]Final model saved to models/curriculum_v2/final_model

Final Stats:
  total_episodes: 1494
  in_warmup: False
  warmup_remaining: 0
  success_rate: 0.21
  success_radius: 2.0
  sampling_weights: {8: 0.3145246382947195, 16: 0.3315700593086727, 32: 0.1981838945138981, 64: 0.15572140788270972}
  per_size_success: {8: 0.2, 16: 0.2, 32: 0.22, 64: 0.16}
  window_fill: 200
